from aiida.engine import WorkChain, ToContext, submit, append_
from aiida.orm import load_group, List, Dict, Code, KpointsData, StructureData, Float, Str, load_node, JsonableData
from aiida.plugins import CalculationFactory
# import UpfData
from aiida.orm import UpfData
import numpy as np
from aiida_quantumespresso.data.hubbard_structure import HubbardStructureData
from lordcapulet.utils import extract_occupations_from_calc
# load group

PwCalculation = CalculationFactory('quantumespresso.pw')

class AFMScanWorkChain(WorkChain):
    @classmethod
    def define(cls, spec):
        super().define(spec)
        # Accept both StructureData and HubbardStructureData
        spec.input('structure', valid_type=(StructureData, HubbardStructureData))
        spec.input('parameters', valid_type=Dict)
        spec.input('kpoints', valid_type=KpointsData)
        # spec.input('pseudos', valid_type=Dict)
        spec.input('code', valid_type=Code)
        spec.input('tm_atoms', valid_type=List)
        spec.input('magnitude', valid_type=Float, default=Float(0.5))
        spec.input('walltime_hours', valid_type=Float, default=lambda: Float(1.0), 
                  help='Walltime in hours for each AFM calculation (default: 1 hour)')  
        spec.outline(
            cls.prepare_configs,
            cls.run_all,
            cls.gather_results,
        )
        spec.output('all_occupation_matrices', valid_type=List)
        spec.output('converged_calculations_pks', valid_type=List)
        spec.output('all_calculation_pks', valid_type=List)

    def prepare_configs(self):
        tm_atoms = self.inputs.tm_atoms.get_list()
        N = len(tm_atoms)
        self.ctx.magnetic_configs = []
        for i in range(2 ** N):
            config = {}
            binary_string = format(i, f'0{N}b')
            for j in range(N):
                config[tm_atoms[j]] = self.inputs.magnitude * (1 if binary_string[j] == '1' else -1)
            self.ctx.magnetic_configs.append(config)
        self.ctx.results = []

    def run_all(self):
        self.ctx.calc_futures = []
        for starting_magnetization in self.ctx.magnetic_configs:
            builder = PwCalculation.get_builder()
            builder.code = self.inputs.code
            builder.structure = self.inputs.structure
            builder.parameters = self.inputs.parameters.clone()
            builder.kpoints = self.inputs.kpoints

            # this is hardcoded for now, needs to be improved 
            pseudo_family = load_group('SSSP/1.3/PBEsol/efficiency')
            pseudos = pseudo_family.get_pseudos(structure=builder.structure)
            # builder.pseudos = self.inputs.pseudos 
            builder.pseudos = pseudos

            builder.parameters['SYSTEM']['starting_magnetization'] = starting_magnetization
            
            # Set metadata for calculations with configurable walltime
            walltime_hours = self.inputs.walltime_hours.value
            walltime_str = f"{int(walltime_hours):02d}:{int((walltime_hours % 1) * 60):02d}:00"
            builder.metadata = {
                'options': {
                    'resources': {'num_machines': 1}, 
                    'withmpi': True,
                    'max_wallclock_seconds': int(walltime_hours * 3600)
                }
            }
            
            # <<< CORRECT KEY FOR OCCUPATION MATRICES >>>
            builder.settings = Dict(dict={'parser_options': {'parse_atomic_occupations': True}})
            # self.ctx.calc_futures.append(self.submit(builder))
            self.to_context(calcs=append_(self.submit(builder)))
        # return ToContext(calcs=self.ctx.calc_futures)

    # def gather_results(self):
    #     matrices = []
    #     for calc in self.ctx.calcs:
    #         if 'output_atomic_occupations' in calc.outputs:
    #             uuid = str(calc.outputs.output_atomic_occupations.uuid)
    #         else:
    #             uuid = 'no matrix'
    #         matrices.append(uuid)
    #     self.out('all_occupation_matrices', List(list=matrices).store())


    # here one needs to also check if the calculation
    # ends with any exit code other than 0
    # if so, we should not store the occupation matrix
    # def gather_results(self):
    #     matrices = []
    #     for calc in self.ctx.calcs:
    #         if 'output_atomic_occupations' in calc.outputs and calc.exit_status == 0:
    #             pk = calc.outputs.output_atomic_occupations.pk
    #         else:
    #             pk = -1  # Or any sentinel value you prefer for "no matrix"
    #             self.report(f"Calculation {i+1} completed but no occupation matrix found")
    #         matrices.append(pk)
    #     self.out('all_occupation_matrices', List(list=matrices).store())
    
    def gather_results(self):
        """
        Collect the PKs and occupation matrices from all calculations.
        """
        converged_calculations_pks = []
        calculation_pks = []
        occupation_matrices_pks = []
        
        self.report(f"DEBUG: gather_results called with {len(self.ctx.calcs)} calculations")
        
        for i, calc in enumerate(self.ctx.calcs):
            calculation_pks.append(calc.pk)
            
            # Reload the calculation node to get fresh state
            fresh_calc = load_node(calc.pk)
            
            # Debug information about calculation state (both cached and fresh)
            self.report(f"DEBUG: Calc {i+1} (PK {calc.pk}):")
            self.report(f"  Cached: is_finished={calc.is_finished}, exit_status={calc.exit_status}, exit_code={calc.exit_code}")
            self.report(f"  Fresh:  is_finished={fresh_calc.is_finished}, exit_status={fresh_calc.exit_status}, exit_code={fresh_calc.exit_code}")

            # Use the fresh node for checking status (use exit_status which is always available)
            if fresh_calc.is_finished and fresh_calc.exit_status == 0:
                converged_calculations_pks.append(calc.pk)
                self.report(f"Calculation {i+1} completed successfully, PK: {calc.pk}")
                
                # Extract and store occupation matrix using unified structure
                try:
                    occupation_data = extract_occupations_from_calc(fresh_calc)
                    # Store as AiiDA JsonableData node directly
                    occ_node = JsonableData(occupation_data)
                    occ_node.store()
                    occupation_matrices_pks.append(occ_node.pk)
                    
                    # Add occupation matrix pk to the extras of each calculation
                    fresh_calc.base.extras.set('occupation_matrix_pk', occ_node.pk)

                    
                    self.report(f"Occupation matrix extracted and stored with PK: {occ_node.pk}")
                except Exception as e:
                    self.report(f"Failed to extract occupation matrix from calculation {calc.pk}: {e}")
            elif fresh_calc.is_finished:
                self.report(f"Calculation {i+1} finished but failed, PK: {calc.pk}, exit status: {fresh_calc.exit_status}")
            else:
                self.report(f"Calculation {i+1} not yet finished, PK: {calc.pk}")
        
        # Store outputs
        self.out('converged_calculations_pks', List(list=converged_calculations_pks).store())
        self.out('all_calculation_pks', List(list=calculation_pks).store())
        self.out('all_occupation_matrices', List(list=occupation_matrices_pks).store())
        
        successful_extractions = len([pk for pk in occupation_matrices_pks if pk != -1])
        self.report(f"Magnetic scan completed. {len(converged_calculations_pks)}/{len(calculation_pks)} calculations converged, {successful_extractions}/{len(calculation_pks)} occupation matrices extracted")